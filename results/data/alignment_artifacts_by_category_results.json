{
  "summary": {
    "mean_peak_cohens_d": 3.3734375,
    "std_peak_cohens_d": 1.1712637728896056,
    "max_peak_cohens_d": 5.4140625,
    "min_peak_cohens_d": 2.095703125
  },
  "category_peaks": {
    "technical_dangerous": {
      "layer": 1,
      "cohens_d": 3.306640625,
      "metrics": {
        "cohens_d": 3.306640625,
        "accuracy": 0.5375,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.1904296875,
        "mean_separation": 1.416015625
      }
    },
    "social_political": {
      "layer": 1,
      "cohens_d": 2.095703125,
      "metrics": {
        "cohens_d": 2.095703125,
        "accuracy": 0.535,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.77001953125,
        "mean_separation": 0.5927734375
      }
    },
    "personal_harmful": {
      "layer": 1,
      "cohens_d": 3.6640625,
      "metrics": {
        "cohens_d": 3.6640625,
        "accuracy": 0.5325,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.2900390625,
        "mean_separation": 1.6650390625
      }
    },
    "medical_ethics": {
      "layer": 1,
      "cohens_d": 5.4140625,
      "metrics": {
        "cohens_d": 5.4140625,
        "accuracy": 0.5475,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.9189453125,
        "mean_separation": 3.681640625
      }
    },
    "information_deception": {
      "layer": 15,
      "cohens_d": 2.38671875,
      "metrics": {
        "cohens_d": 2.38671875,
        "accuracy": 0.8575,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.44775390625,
        "mean_separation": 0.200439453125
      }
    }
  },
  "results_by_layer": {
    "0": {
      "technical_dangerous": {
        "cohens_d": 2.53125,
        "accuracy": 0.615,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.4404296875,
        "mean_separation": 2.07421875
      },
      "social_political": {
        "cohens_d": 1.837890625,
        "accuracy": 0.5775,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.0546875,
        "mean_separation": 1.1123046875
      },
      "personal_harmful": {
        "cohens_d": 3.41796875,
        "accuracy": 0.6175,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.9541015625,
        "mean_separation": 3.8203125
      },
      "medical_ethics": {
        "cohens_d": 3.8515625,
        "accuracy": 0.65,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 2.169921875,
        "mean_separation": 4.7109375
      },
      "information_deception": {
        "cohens_d": 2.158203125,
        "accuracy": 0.665,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.2119140625,
        "mean_separation": 1.46875
      }
    },
    "1": {
      "technical_dangerous": {
        "cohens_d": 3.306640625,
        "accuracy": 0.5375,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.1904296875,
        "mean_separation": 1.416015625
      },
      "social_political": {
        "cohens_d": 2.095703125,
        "accuracy": 0.535,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.77001953125,
        "mean_separation": 0.5927734375
      },
      "personal_harmful": {
        "cohens_d": 3.6640625,
        "accuracy": 0.5325,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.2900390625,
        "mean_separation": 1.6650390625
      },
      "medical_ethics": {
        "cohens_d": 5.4140625,
        "accuracy": 0.5475,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.9189453125,
        "mean_separation": 3.681640625
      },
      "information_deception": {
        "cohens_d": 2.126953125,
        "accuracy": 0.53,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.74658203125,
        "mean_separation": 0.5576171875
      }
    },
    "2": {
      "technical_dangerous": {
        "cohens_d": 2.12109375,
        "accuracy": 0.5175,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.6982421875,
        "mean_separation": 0.48779296875
      },
      "social_political": {
        "cohens_d": 1.625,
        "accuracy": 0.54,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.53662109375,
        "mean_separation": 0.28759765625
      },
      "personal_harmful": {
        "cohens_d": 2.876953125,
        "accuracy": 0.625,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.9423828125,
        "mean_separation": 0.88818359375
      },
      "medical_ethics": {
        "cohens_d": 4.0078125,
        "accuracy": 0.5525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.326171875,
        "mean_separation": 1.7578125
      },
      "information_deception": {
        "cohens_d": 2.109375,
        "accuracy": 0.56,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.69482421875,
        "mean_separation": 0.48291015625
      }
    },
    "3": {
      "technical_dangerous": {
        "cohens_d": 1.3515625,
        "accuracy": 0.6225,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.36572265625,
        "mean_separation": 0.133544921875
      },
      "social_political": {
        "cohens_d": 1.1123046875,
        "accuracy": 0.6175,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.30517578125,
        "mean_separation": 0.0931396484375
      },
      "personal_harmful": {
        "cohens_d": 2.11328125,
        "accuracy": 0.7025,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.5654296875,
        "mean_separation": 0.320068359375
      },
      "medical_ethics": {
        "cohens_d": 2.208984375,
        "accuracy": 0.735,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.59130859375,
        "mean_separation": 0.349365234375
      },
      "information_deception": {
        "cohens_d": 1.5849609375,
        "accuracy": 0.5475,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.42138671875,
        "mean_separation": 0.177734375
      }
    },
    "4": {
      "technical_dangerous": {
        "cohens_d": 1.3330078125,
        "accuracy": 0.7125,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.266357421875,
        "mean_separation": 0.0709228515625
      },
      "social_political": {
        "cohens_d": 1.2119140625,
        "accuracy": 0.7775,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2432861328125,
        "mean_separation": 0.0592041015625
      },
      "personal_harmful": {
        "cohens_d": 2.2578125,
        "accuracy": 0.8075,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.4462890625,
        "mean_separation": 0.19921875
      },
      "medical_ethics": {
        "cohens_d": 2.361328125,
        "accuracy": 0.9275,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.461181640625,
        "mean_separation": 0.2127685546875
      },
      "information_deception": {
        "cohens_d": 1.439453125,
        "accuracy": 0.78,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.28369140625,
        "mean_separation": 0.08050537109375
      }
    },
    "5": {
      "technical_dangerous": {
        "cohens_d": 1.525390625,
        "accuracy": 0.7025,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.3818359375,
        "mean_separation": 0.145751953125
      },
      "social_political": {
        "cohens_d": 1.4248046875,
        "accuracy": 0.7575,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.355224609375,
        "mean_separation": 0.126220703125
      },
      "personal_harmful": {
        "cohens_d": 2.46875,
        "accuracy": 0.715,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.6123046875,
        "mean_separation": 0.374755859375
      },
      "medical_ethics": {
        "cohens_d": 2.78515625,
        "accuracy": 0.715,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.70361328125,
        "mean_separation": 0.494873046875
      },
      "information_deception": {
        "cohens_d": 1.80859375,
        "accuracy": 0.6225,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.443603515625,
        "mean_separation": 0.19677734375
      }
    },
    "6": {
      "technical_dangerous": {
        "cohens_d": 2.08203125,
        "accuracy": 0.7125,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.54296875,
        "mean_separation": 0.294921875
      },
      "social_political": {
        "cohens_d": 1.7646484375,
        "accuracy": 0.7775,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.46435546875,
        "mean_separation": 0.215576171875
      },
      "personal_harmful": {
        "cohens_d": 3.50390625,
        "accuracy": 0.855,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.90087890625,
        "mean_separation": 0.81201171875
      },
      "medical_ethics": {
        "cohens_d": 3.869140625,
        "accuracy": 0.89,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 1.013671875,
        "mean_separation": 1.02734375
      },
      "information_deception": {
        "cohens_d": 2.1640625,
        "accuracy": 0.7675,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.5625,
        "mean_separation": 0.316162109375
      }
    },
    "7": {
      "technical_dangerous": {
        "cohens_d": 1.8525390625,
        "accuracy": 0.685,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.369140625,
        "mean_separation": 0.13623046875
      },
      "social_political": {
        "cohens_d": 1.3935546875,
        "accuracy": 0.8525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.281005859375,
        "mean_separation": 0.07891845703125
      },
      "personal_harmful": {
        "cohens_d": 2.974609375,
        "accuracy": 0.8525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.58642578125,
        "mean_separation": 0.34423828125
      },
      "medical_ethics": {
        "cohens_d": 3.228515625,
        "accuracy": 0.8725,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.6357421875,
        "mean_separation": 0.404541015625
      },
      "information_deception": {
        "cohens_d": 1.9052734375,
        "accuracy": 0.7425,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.377197265625,
        "mean_separation": 0.14208984375
      }
    },
    "8": {
      "technical_dangerous": {
        "cohens_d": 1.513671875,
        "accuracy": 0.715,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.216552734375,
        "mean_separation": 0.046875
      },
      "social_political": {
        "cohens_d": 1.296875,
        "accuracy": 0.85,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.18701171875,
        "mean_separation": 0.03497314453125
      },
      "personal_harmful": {
        "cohens_d": 2.166015625,
        "accuracy": 0.8825,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.308837890625,
        "mean_separation": 0.0953369140625
      },
      "medical_ethics": {
        "cohens_d": 2.439453125,
        "accuracy": 0.8925,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.3525390625,
        "mean_separation": 0.124267578125
      },
      "information_deception": {
        "cohens_d": 1.48046875,
        "accuracy": 0.8225,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.20947265625,
        "mean_separation": 0.04388427734375
      }
    },
    "9": {
      "technical_dangerous": {
        "cohens_d": 2.0546875,
        "accuracy": 0.8225,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.406005859375,
        "mean_separation": 0.164794921875
      },
      "social_political": {
        "cohens_d": 1.9482421875,
        "accuracy": 0.8925,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.3916015625,
        "mean_separation": 0.1533203125
      },
      "personal_harmful": {
        "cohens_d": 2.685546875,
        "accuracy": 0.91,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.5400390625,
        "mean_separation": 0.291748046875
      },
      "medical_ethics": {
        "cohens_d": 3.283203125,
        "accuracy": 0.935,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.6591796875,
        "mean_separation": 0.43505859375
      },
      "information_deception": {
        "cohens_d": 2.224609375,
        "accuracy": 0.8675,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.4384765625,
        "mean_separation": 0.1922607421875
      }
    },
    "10": {
      "technical_dangerous": {
        "cohens_d": 1.6640625,
        "accuracy": 0.82,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.268798828125,
        "mean_separation": 0.072265625
      },
      "social_political": {
        "cohens_d": 1.5693359375,
        "accuracy": 0.9275,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.25634765625,
        "mean_separation": 0.0657958984375
      },
      "personal_harmful": {
        "cohens_d": 2.3359375,
        "accuracy": 0.9525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.375244140625,
        "mean_separation": 0.140869140625
      },
      "medical_ethics": {
        "cohens_d": 2.681640625,
        "accuracy": 0.9725,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.435302734375,
        "mean_separation": 0.1895751953125
      },
      "information_deception": {
        "cohens_d": 1.8251953125,
        "accuracy": 0.85,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.293212890625,
        "mean_separation": 0.08599853515625
      }
    },
    "11": {
      "technical_dangerous": {
        "cohens_d": 2.029296875,
        "accuracy": 0.86,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.34619140625,
        "mean_separation": 0.119873046875
      },
      "social_political": {
        "cohens_d": 1.85546875,
        "accuracy": 0.955,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.32080078125,
        "mean_separation": 0.10284423828125
      },
      "personal_harmful": {
        "cohens_d": 2.685546875,
        "accuracy": 0.9775,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.4580078125,
        "mean_separation": 0.209716796875
      },
      "medical_ethics": {
        "cohens_d": 3.0234375,
        "accuracy": 0.98,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.52587890625,
        "mean_separation": 0.2763671875
      },
      "information_deception": {
        "cohens_d": 2.068359375,
        "accuracy": 0.88,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.351318359375,
        "mean_separation": 0.1234130859375
      }
    },
    "12": {
      "technical_dangerous": {
        "cohens_d": 1.4150390625,
        "accuracy": 0.86,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.1646728515625,
        "mean_separation": 0.027099609375
      },
      "social_political": {
        "cohens_d": 1.2109375,
        "accuracy": 0.945,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.1434326171875,
        "mean_separation": 0.02056884765625
      },
      "personal_harmful": {
        "cohens_d": 1.9345703125,
        "accuracy": 0.9575,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.226806640625,
        "mean_separation": 0.051422119140625
      },
      "medical_ethics": {
        "cohens_d": 2.060546875,
        "accuracy": 0.9525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.243896484375,
        "mean_separation": 0.05950927734375
      },
      "information_deception": {
        "cohens_d": 1.48046875,
        "accuracy": 0.885,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.173828125,
        "mean_separation": 0.0301971435546875
      }
    },
    "13": {
      "technical_dangerous": {
        "cohens_d": 1.7802734375,
        "accuracy": 0.85,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2344970703125,
        "mean_separation": 0.05499267578125
      },
      "social_political": {
        "cohens_d": 1.5087890625,
        "accuracy": 0.9375,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2022705078125,
        "mean_separation": 0.0408935546875
      },
      "personal_harmful": {
        "cohens_d": 2.376953125,
        "accuracy": 0.925,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.31298828125,
        "mean_separation": 0.0980224609375
      },
      "medical_ethics": {
        "cohens_d": 2.544921875,
        "accuracy": 0.915,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.337890625,
        "mean_separation": 0.11407470703125
      },
      "information_deception": {
        "cohens_d": 1.798828125,
        "accuracy": 0.84,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2374267578125,
        "mean_separation": 0.056365966796875
      }
    },
    "14": {
      "technical_dangerous": {
        "cohens_d": 1.5810546875,
        "accuracy": 0.845,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2158203125,
        "mean_separation": 0.046600341796875
      },
      "social_political": {
        "cohens_d": 1.4150390625,
        "accuracy": 0.915,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.1976318359375,
        "mean_separation": 0.0390625
      },
      "personal_harmful": {
        "cohens_d": 2.046875,
        "accuracy": 0.92,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2802734375,
        "mean_separation": 0.07855224609375
      },
      "medical_ethics": {
        "cohens_d": 2.25,
        "accuracy": 0.925,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.310302734375,
        "mean_separation": 0.0963134765625
      },
      "information_deception": {
        "cohens_d": 1.634765625,
        "accuracy": 0.835,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2222900390625,
        "mean_separation": 0.0494384765625
      }
    },
    "15": {
      "technical_dangerous": {
        "cohens_d": 2.208984375,
        "accuracy": 0.9025,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.41552734375,
        "mean_separation": 0.1728515625
      },
      "social_political": {
        "cohens_d": 2.005859375,
        "accuracy": 0.9325,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.385498046875,
        "mean_separation": 0.148681640625
      },
      "personal_harmful": {
        "cohens_d": 2.984375,
        "accuracy": 0.8975,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.55859375,
        "mean_separation": 0.311767578125
      },
      "medical_ethics": {
        "cohens_d": 3.32421875,
        "accuracy": 0.9,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.62646484375,
        "mean_separation": 0.392578125
      },
      "information_deception": {
        "cohens_d": 2.38671875,
        "accuracy": 0.8575,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.44775390625,
        "mean_separation": 0.200439453125
      }
    },
    "16": {
      "technical_dangerous": {
        "cohens_d": 2.16796875,
        "accuracy": 0.87,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.464111328125,
        "mean_separation": 0.2154541015625
      },
      "social_political": {
        "cohens_d": 1.8359375,
        "accuracy": 0.89,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.397216796875,
        "mean_separation": 0.15771484375
      },
      "personal_harmful": {
        "cohens_d": 2.984375,
        "accuracy": 0.92,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.63427734375,
        "mean_separation": 0.402587890625
      },
      "medical_ethics": {
        "cohens_d": 3.181640625,
        "accuracy": 0.9075,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.67529296875,
        "mean_separation": 0.45556640625
      },
      "information_deception": {
        "cohens_d": 2.2578125,
        "accuracy": 0.81,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.478515625,
        "mean_separation": 0.2291259765625
      }
    },
    "17": {
      "technical_dangerous": {
        "cohens_d": 1.9130859375,
        "accuracy": 0.8325,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.4130859375,
        "mean_separation": 0.170654296875
      },
      "social_political": {
        "cohens_d": 1.7705078125,
        "accuracy": 0.895,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.384521484375,
        "mean_separation": 0.14794921875
      },
      "personal_harmful": {
        "cohens_d": 2.875,
        "accuracy": 0.87,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.6171875,
        "mean_separation": 0.381103515625
      },
      "medical_ethics": {
        "cohens_d": 2.94921875,
        "accuracy": 0.85,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.63232421875,
        "mean_separation": 0.39990234375
      },
      "information_deception": {
        "cohens_d": 2.01171875,
        "accuracy": 0.7525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.430419921875,
        "mean_separation": 0.18505859375
      }
    },
    "18": {
      "technical_dangerous": {
        "cohens_d": 1.4130859375,
        "accuracy": 0.79,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.232177734375,
        "mean_separation": 0.05389404296875
      },
      "social_political": {
        "cohens_d": 1.365234375,
        "accuracy": 0.835,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.228515625,
        "mean_separation": 0.05224609375
      },
      "personal_harmful": {
        "cohens_d": 2.2578125,
        "accuracy": 0.8025,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.374267578125,
        "mean_separation": 0.14013671875
      },
      "medical_ethics": {
        "cohens_d": 2.439453125,
        "accuracy": 0.8,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.4072265625,
        "mean_separation": 0.165771484375
      },
      "information_deception": {
        "cohens_d": 1.6748046875,
        "accuracy": 0.7,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.274169921875,
        "mean_separation": 0.0751953125
      }
    },
    "19": {
      "technical_dangerous": {
        "cohens_d": 1.4150390625,
        "accuracy": 0.765,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.25048828125,
        "mean_separation": 0.062744140625
      },
      "social_political": {
        "cohens_d": 1.3427734375,
        "accuracy": 0.6725,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.242919921875,
        "mean_separation": 0.0589599609375
      },
      "personal_harmful": {
        "cohens_d": 2.3203125,
        "accuracy": 0.745,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.417724609375,
        "mean_separation": 0.1741943359375
      },
      "medical_ethics": {
        "cohens_d": 2.62109375,
        "accuracy": 0.7275,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.47314453125,
        "mean_separation": 0.223876953125
      },
      "information_deception": {
        "cohens_d": 1.8134765625,
        "accuracy": 0.635,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.31787109375,
        "mean_separation": 0.10107421875
      }
    },
    "20": {
      "technical_dangerous": {
        "cohens_d": 1.7734375,
        "accuracy": 0.775,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.38134765625,
        "mean_separation": 0.1455078125
      },
      "social_political": {
        "cohens_d": 1.935546875,
        "accuracy": 0.6525,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.427001953125,
        "mean_separation": 0.1826171875
      },
      "personal_harmful": {
        "cohens_d": 2.76953125,
        "accuracy": 0.78,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.61865234375,
        "mean_separation": 0.383056640625
      },
      "medical_ethics": {
        "cohens_d": 3.115234375,
        "accuracy": 0.73,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.693359375,
        "mean_separation": 0.4814453125
      },
      "information_deception": {
        "cohens_d": 2.193359375,
        "accuracy": 0.6475,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.47021484375,
        "mean_separation": 0.2216796875
      }
    },
    "21": {
      "technical_dangerous": {
        "cohens_d": 1.4599609375,
        "accuracy": 0.7825,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.25,
        "mean_separation": 0.06256103515625
      },
      "social_political": {
        "cohens_d": 1.46484375,
        "accuracy": 0.6625,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.25390625,
        "mean_separation": 0.064453125
      },
      "personal_harmful": {
        "cohens_d": 2.20703125,
        "accuracy": 0.805,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.3974609375,
        "mean_separation": 0.1580810546875
      },
      "medical_ethics": {
        "cohens_d": 2.515625,
        "accuracy": 0.7675,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.45166015625,
        "mean_separation": 0.203857421875
      },
      "information_deception": {
        "cohens_d": 1.609375,
        "accuracy": 0.68,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.273681640625,
        "mean_separation": 0.0750732421875
      }
    },
    "22": {
      "technical_dangerous": {
        "cohens_d": 1.568359375,
        "accuracy": 0.8025,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.301025390625,
        "mean_separation": 0.0906982421875
      },
      "social_political": {
        "cohens_d": 1.732421875,
        "accuracy": 0.6425,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.33447265625,
        "mean_separation": 0.11181640625
      },
      "personal_harmful": {
        "cohens_d": 2.529296875,
        "accuracy": 0.7675,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.5068359375,
        "mean_separation": 0.257080078125
      },
      "medical_ethics": {
        "cohens_d": 2.638671875,
        "accuracy": 0.79,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.529296875,
        "mean_separation": 0.280029296875
      },
      "information_deception": {
        "cohens_d": 2.001953125,
        "accuracy": 0.6575,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.38037109375,
        "mean_separation": 0.14453125
      }
    },
    "23": {
      "technical_dangerous": {
        "cohens_d": 1.423828125,
        "accuracy": 0.8475,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2330322265625,
        "mean_separation": 0.0543212890625
      },
      "social_political": {
        "cohens_d": 1.4775390625,
        "accuracy": 0.7375,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.2452392578125,
        "mean_separation": 0.06005859375
      },
      "personal_harmful": {
        "cohens_d": 2.1953125,
        "accuracy": 0.795,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.375,
        "mean_separation": 0.1405029296875
      },
      "medical_ethics": {
        "cohens_d": 2.310546875,
        "accuracy": 0.82,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.395263671875,
        "mean_separation": 0.15625
      },
      "information_deception": {
        "cohens_d": 1.650390625,
        "accuracy": 0.72,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.27197265625,
        "mean_separation": 0.0738525390625
      }
    },
    "24": {
      "technical_dangerous": {
        "cohens_d": 1.205078125,
        "accuracy": 0.8175,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.16796875,
        "mean_separation": 0.0282135009765625
      },
      "social_political": {
        "cohens_d": 1.21875,
        "accuracy": 0.7025,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.1717529296875,
        "mean_separation": 0.02947998046875
      },
      "personal_harmful": {
        "cohens_d": 1.9375,
        "accuracy": 0.805,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.279296875,
        "mean_separation": 0.07794189453125
      },
      "medical_ethics": {
        "cohens_d": 1.943359375,
        "accuracy": 0.8425,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.282958984375,
        "mean_separation": 0.080078125
      },
      "information_deception": {
        "cohens_d": 1.3388671875,
        "accuracy": 0.72,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.1875,
        "mean_separation": 0.03515625
      }
    },
    "25": {
      "technical_dangerous": {
        "cohens_d": 2.03125,
        "accuracy": 0.8425,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.438720703125,
        "mean_separation": 0.1923828125
      },
      "social_political": {
        "cohens_d": 1.955078125,
        "accuracy": 0.75,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.4296875,
        "mean_separation": 0.184814453125
      },
      "personal_harmful": {
        "cohens_d": 2.96875,
        "accuracy": 0.8275,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.6611328125,
        "mean_separation": 0.43701171875
      },
      "medical_ethics": {
        "cohens_d": 3.150390625,
        "accuracy": 0.89,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.71484375,
        "mean_separation": 0.51123046875
      },
      "information_deception": {
        "cohens_d": 1.9501953125,
        "accuracy": 0.7775,
        "n_natural": 200,
        "n_artifact": 200,
        "direction_norm": 0.41943359375,
        "mean_separation": 0.176025390625
      }
    }
  },
  "config": {
    "num_layers": 26,
    "key_pattern": "model.layers.{layer}.mlp.output",
    "num_steps": 20
  }
}