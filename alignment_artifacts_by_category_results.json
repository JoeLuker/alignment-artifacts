{
  "summary": {
    "mean_peak_cohens_d": 3.433984375,
    "std_peak_cohens_d": 0.7965497957745249,
    "max_peak_cohens_d": 4.40625,
    "min_peak_cohens_d": 2.26171875,
    "hypothesis_test": {
      "political_d": 2.7578125,
      "harmfulness_d": 4.40625,
      "hypothesis_supported": false
    }
  },
  "category_peaks": {
    "technical_dangerous": {
      "layer": 1,
      "cohens_d": 3.794921875,
      "metrics": {
        "cohens_d": 3.794921875,
        "accuracy": 0.55,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.3583984375,
        "mean_separation": 1.845703125
      }
    },
    "social_political": {
      "layer": 1,
      "cohens_d": 2.7578125,
      "metrics": {
        "cohens_d": 2.7578125,
        "accuracy": 0.535,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.99755859375,
        "mean_separation": 0.9951171875
      }
    },
    "personal_harmful": {
      "layer": 1,
      "cohens_d": 4.40625,
      "metrics": {
        "cohens_d": 4.40625,
        "accuracy": 0.555,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.52734375,
        "mean_separation": 2.330078125
      }
    },
    "medical_ethics": {
      "layer": 2,
      "cohens_d": 3.94921875,
      "metrics": {
        "cohens_d": 3.94921875,
        "accuracy": 0.585,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.287109375,
        "mean_separation": 1.6552734375
      }
    },
    "information_deception": {
      "layer": 0,
      "cohens_d": 2.26171875,
      "metrics": {
        "cohens_d": 2.26171875,
        "accuracy": 0.565,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.271484375,
        "mean_separation": 1.6171875
      }
    }
  },
  "results_by_layer": {
    "0": {
      "technical_dangerous": {
        "cohens_d": 3.083984375,
        "accuracy": 0.715,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.7412109375,
        "mean_separation": 3.03125
      },
      "social_political": {
        "cohens_d": 2.107421875,
        "accuracy": 0.67,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.1962890625,
        "mean_separation": 1.431640625
      },
      "personal_harmful": {
        "cohens_d": 3.8046875,
        "accuracy": 0.645,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 2.158203125,
        "mean_separation": 4.6640625
      },
      "medical_ethics": {
        "cohens_d": 3.763671875,
        "accuracy": 0.77,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 2.115234375,
        "mean_separation": 4.47265625
      },
      "information_deception": {
        "cohens_d": 2.26171875,
        "accuracy": 0.565,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.271484375,
        "mean_separation": 1.6171875
      }
    },
    "1": {
      "technical_dangerous": {
        "cohens_d": 3.794921875,
        "accuracy": 0.55,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.3583984375,
        "mean_separation": 1.845703125
      },
      "social_political": {
        "cohens_d": 2.7578125,
        "accuracy": 0.535,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.99755859375,
        "mean_separation": 0.9951171875
      },
      "personal_harmful": {
        "cohens_d": 4.40625,
        "accuracy": 0.555,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.52734375,
        "mean_separation": 2.330078125
      },
      "medical_ethics": {
        "cohens_d": 3.857421875,
        "accuracy": 0.54,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.3740234375,
        "mean_separation": 1.8876953125
      },
      "information_deception": {
        "cohens_d": 1.6396484375,
        "accuracy": 0.52,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.57763671875,
        "mean_separation": 0.333740234375
      }
    },
    "2": {
      "technical_dangerous": {
        "cohens_d": 3.130859375,
        "accuracy": 0.565,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.0244140625,
        "mean_separation": 1.048828125
      },
      "social_political": {
        "cohens_d": 2.333984375,
        "accuracy": 0.56,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.7685546875,
        "mean_separation": 0.59033203125
      },
      "personal_harmful": {
        "cohens_d": 2.8828125,
        "accuracy": 0.5,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.94482421875,
        "mean_separation": 0.89306640625
      },
      "medical_ethics": {
        "cohens_d": 3.94921875,
        "accuracy": 0.585,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 1.287109375,
        "mean_separation": 1.6552734375
      },
      "information_deception": {
        "cohens_d": 1.9912109375,
        "accuracy": 0.515,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.65478515625,
        "mean_separation": 0.42822265625
      }
    },
    "3": {
      "technical_dangerous": {
        "cohens_d": 1.8515625,
        "accuracy": 0.635,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.490478515625,
        "mean_separation": 0.24072265625
      },
      "social_political": {
        "cohens_d": 1.265625,
        "accuracy": 0.54,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.34423828125,
        "mean_separation": 0.1185302734375
      },
      "personal_harmful": {
        "cohens_d": 2.10546875,
        "accuracy": 0.64,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.5634765625,
        "mean_separation": 0.3173828125
      },
      "medical_ethics": {
        "cohens_d": 2.62109375,
        "accuracy": 0.625,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.69287109375,
        "mean_separation": 0.479736328125
      },
      "information_deception": {
        "cohens_d": 1.708984375,
        "accuracy": 0.53,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.454833984375,
        "mean_separation": 0.206787109375
      }
    },
    "4": {
      "technical_dangerous": {
        "cohens_d": 1.7216796875,
        "accuracy": 0.8,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.33935546875,
        "mean_separation": 0.11517333984375
      },
      "social_political": {
        "cohens_d": 1.373046875,
        "accuracy": 0.84,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.269775390625,
        "mean_separation": 0.0726318359375
      },
      "personal_harmful": {
        "cohens_d": 2.41796875,
        "accuracy": 0.785,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.4794921875,
        "mean_separation": 0.22998046875
      },
      "medical_ethics": {
        "cohens_d": 2.416015625,
        "accuracy": 0.875,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.469970703125,
        "mean_separation": 0.220947265625
      },
      "information_deception": {
        "cohens_d": 1.259765625,
        "accuracy": 0.705,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2484130859375,
        "mean_separation": 0.0616455078125
      }
    },
    "5": {
      "technical_dangerous": {
        "cohens_d": 2.068359375,
        "accuracy": 0.71,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.513671875,
        "mean_separation": 0.263916015625
      },
      "social_political": {
        "cohens_d": 1.5380859375,
        "accuracy": 0.76,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.379150390625,
        "mean_separation": 0.1436767578125
      },
      "personal_harmful": {
        "cohens_d": 2.412109375,
        "accuracy": 0.895,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.59228515625,
        "mean_separation": 0.350830078125
      },
      "medical_ethics": {
        "cohens_d": 2.939453125,
        "accuracy": 0.71,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.732421875,
        "mean_separation": 0.537109375
      },
      "information_deception": {
        "cohens_d": 1.67578125,
        "accuracy": 0.53,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.410888671875,
        "mean_separation": 0.1688232421875
      }
    },
    "6": {
      "technical_dangerous": {
        "cohens_d": 2.82421875,
        "accuracy": 0.765,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.7333984375,
        "mean_separation": 0.537109375
      },
      "social_political": {
        "cohens_d": 2.044921875,
        "accuracy": 0.795,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.5380859375,
        "mean_separation": 0.289794921875
      },
      "personal_harmful": {
        "cohens_d": 3.55859375,
        "accuracy": 0.87,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.9189453125,
        "mean_separation": 0.8447265625
      },
      "medical_ethics": {
        "cohens_d": 3.904296875,
        "accuracy": 0.885,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.9970703125,
        "mean_separation": 0.994140625
      },
      "information_deception": {
        "cohens_d": 1.779296875,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.463134765625,
        "mean_separation": 0.2144775390625
      }
    },
    "7": {
      "technical_dangerous": {
        "cohens_d": 2.517578125,
        "accuracy": 0.75,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.4970703125,
        "mean_separation": 0.246826171875
      },
      "social_political": {
        "cohens_d": 1.6220703125,
        "accuracy": 0.735,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.324462890625,
        "mean_separation": 0.1053466796875
      },
      "personal_harmful": {
        "cohens_d": 2.9453125,
        "accuracy": 0.855,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.5791015625,
        "mean_separation": 0.33544921875
      },
      "medical_ethics": {
        "cohens_d": 3.32421875,
        "accuracy": 0.84,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.64599609375,
        "mean_separation": 0.41650390625
      },
      "information_deception": {
        "cohens_d": 1.5830078125,
        "accuracy": 0.76,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.3134765625,
        "mean_separation": 0.098388671875
      }
    },
    "8": {
      "technical_dangerous": {
        "cohens_d": 1.912109375,
        "accuracy": 0.775,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2705078125,
        "mean_separation": 0.0731201171875
      },
      "social_political": {
        "cohens_d": 1.4248046875,
        "accuracy": 0.855,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2034912109375,
        "mean_separation": 0.041412353515625
      },
      "personal_harmful": {
        "cohens_d": 2.251953125,
        "accuracy": 0.875,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.31787109375,
        "mean_separation": 0.1009521484375
      },
      "medical_ethics": {
        "cohens_d": 2.56640625,
        "accuracy": 0.87,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.362548828125,
        "mean_separation": 0.131591796875
      },
      "information_deception": {
        "cohens_d": 1.232421875,
        "accuracy": 0.865,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.17431640625,
        "mean_separation": 0.0303802490234375
      }
    },
    "9": {
      "technical_dangerous": {
        "cohens_d": 2.70703125,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.525390625,
        "mean_separation": 0.276123046875
      },
      "social_political": {
        "cohens_d": 2.076171875,
        "accuracy": 0.935,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.413818359375,
        "mean_separation": 0.171142578125
      },
      "personal_harmful": {
        "cohens_d": 3.0234375,
        "accuracy": 0.895,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.591796875,
        "mean_separation": 0.350341796875
      },
      "medical_ethics": {
        "cohens_d": 3.52734375,
        "accuracy": 0.9,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.681640625,
        "mean_separation": 0.46435546875
      },
      "information_deception": {
        "cohens_d": 1.9541015625,
        "accuracy": 0.875,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.38623046875,
        "mean_separation": 0.149169921875
      }
    },
    "10": {
      "technical_dangerous": {
        "cohens_d": 2.15234375,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.338623046875,
        "mean_separation": 0.1146240234375
      },
      "social_political": {
        "cohens_d": 1.5908203125,
        "accuracy": 0.91,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.25537109375,
        "mean_separation": 0.065185546875
      },
      "personal_harmful": {
        "cohens_d": 2.521484375,
        "accuracy": 0.945,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.4033203125,
        "mean_separation": 0.16259765625
      },
      "medical_ethics": {
        "cohens_d": 2.708984375,
        "accuracy": 0.93,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.427001953125,
        "mean_separation": 0.1824951171875
      },
      "information_deception": {
        "cohens_d": 1.5439453125,
        "accuracy": 0.8,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.24853515625,
        "mean_separation": 0.061737060546875
      }
    },
    "11": {
      "technical_dangerous": {
        "cohens_d": 2.546875,
        "accuracy": 0.855,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.426513671875,
        "mean_separation": 0.181884765625
      },
      "social_political": {
        "cohens_d": 1.8427734375,
        "accuracy": 0.965,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.317138671875,
        "mean_separation": 0.10064697265625
      },
      "personal_harmful": {
        "cohens_d": 2.8984375,
        "accuracy": 0.95,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.490478515625,
        "mean_separation": 0.240478515625
      },
      "medical_ethics": {
        "cohens_d": 3.208984375,
        "accuracy": 0.945,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.54345703125,
        "mean_separation": 0.29541015625
      },
      "information_deception": {
        "cohens_d": 1.84765625,
        "accuracy": 0.885,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.3115234375,
        "mean_separation": 0.09716796875
      }
    },
    "12": {
      "technical_dangerous": {
        "cohens_d": 1.734375,
        "accuracy": 0.855,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.19677734375,
        "mean_separation": 0.03875732421875
      },
      "social_political": {
        "cohens_d": 1.255859375,
        "accuracy": 0.95,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.1473388671875,
        "mean_separation": 0.021697998046875
      },
      "personal_harmful": {
        "cohens_d": 2.12890625,
        "accuracy": 0.93,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2491455078125,
        "mean_separation": 0.0621337890625
      },
      "medical_ethics": {
        "cohens_d": 2.1875,
        "accuracy": 0.935,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.250732421875,
        "mean_separation": 0.0628662109375
      },
      "information_deception": {
        "cohens_d": 1.328125,
        "accuracy": 0.9,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.1546630859375,
        "mean_separation": 0.0239105224609375
      }
    },
    "13": {
      "technical_dangerous": {
        "cohens_d": 2.26953125,
        "accuracy": 0.88,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.29443359375,
        "mean_separation": 0.08673095703125
      },
      "social_political": {
        "cohens_d": 1.734375,
        "accuracy": 0.955,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2318115234375,
        "mean_separation": 0.053741455078125
      },
      "personal_harmful": {
        "cohens_d": 2.736328125,
        "accuracy": 0.945,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.359130859375,
        "mean_separation": 0.1287841796875
      },
      "medical_ethics": {
        "cohens_d": 2.81640625,
        "accuracy": 0.96,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.369873046875,
        "mean_separation": 0.13671875
      },
      "information_deception": {
        "cohens_d": 1.712890625,
        "accuracy": 0.845,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.224365234375,
        "mean_separation": 0.05035400390625
      }
    },
    "14": {
      "technical_dangerous": {
        "cohens_d": 1.982421875,
        "accuracy": 0.875,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.26806640625,
        "mean_separation": 0.0718994140625
      },
      "social_political": {
        "cohens_d": 1.5166015625,
        "accuracy": 0.965,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2117919921875,
        "mean_separation": 0.04486083984375
      },
      "personal_harmful": {
        "cohens_d": 2.31640625,
        "accuracy": 0.91,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.319580078125,
        "mean_separation": 0.10211181640625
      },
      "medical_ethics": {
        "cohens_d": 2.40625,
        "accuracy": 0.945,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.329345703125,
        "mean_separation": 0.1085205078125
      },
      "information_deception": {
        "cohens_d": 1.494140625,
        "accuracy": 0.865,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.203369140625,
        "mean_separation": 0.0413818359375
      }
    },
    "15": {
      "technical_dangerous": {
        "cohens_d": 2.75390625,
        "accuracy": 0.9,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.51416015625,
        "mean_separation": 0.26416015625
      },
      "social_political": {
        "cohens_d": 2.244140625,
        "accuracy": 0.95,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.43505859375,
        "mean_separation": 0.189453125
      },
      "personal_harmful": {
        "cohens_d": 3.45703125,
        "accuracy": 0.9,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.64794921875,
        "mean_separation": 0.41943359375
      },
      "medical_ethics": {
        "cohens_d": 3.484375,
        "accuracy": 0.915,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.6552734375,
        "mean_separation": 0.42919921875
      },
      "information_deception": {
        "cohens_d": 2.18359375,
        "accuracy": 0.87,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.412841796875,
        "mean_separation": 0.17041015625
      }
    },
    "16": {
      "technical_dangerous": {
        "cohens_d": 2.73828125,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.5849609375,
        "mean_separation": 0.342041015625
      },
      "social_political": {
        "cohens_d": 2.072265625,
        "accuracy": 0.93,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.451171875,
        "mean_separation": 0.20361328125
      },
      "personal_harmful": {
        "cohens_d": 3.34375,
        "accuracy": 0.92,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.71533203125,
        "mean_separation": 0.51171875
      },
      "medical_ethics": {
        "cohens_d": 3.3046875,
        "accuracy": 0.915,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.70703125,
        "mean_separation": 0.499755859375
      },
      "information_deception": {
        "cohens_d": 2.048828125,
        "accuracy": 0.855,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.44140625,
        "mean_separation": 0.1949462890625
      }
    },
    "17": {
      "technical_dangerous": {
        "cohens_d": 2.36328125,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.50634765625,
        "mean_separation": 0.256103515625
      },
      "social_political": {
        "cohens_d": 1.9775390625,
        "accuracy": 0.83,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.428955078125,
        "mean_separation": 0.1839599609375
      },
      "personal_harmful": {
        "cohens_d": 3.369140625,
        "accuracy": 0.845,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.72998046875,
        "mean_separation": 0.53271484375
      },
      "medical_ethics": {
        "cohens_d": 3.076171875,
        "accuracy": 0.9,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.65673828125,
        "mean_separation": 0.430908203125
      },
      "information_deception": {
        "cohens_d": 1.888671875,
        "accuracy": 0.785,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.40771484375,
        "mean_separation": 0.166015625
      }
    },
    "18": {
      "technical_dangerous": {
        "cohens_d": 1.798828125,
        "accuracy": 0.825,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.2939453125,
        "mean_separation": 0.08636474609375
      },
      "social_political": {
        "cohens_d": 1.5869140625,
        "accuracy": 0.795,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.265380859375,
        "mean_separation": 0.0703125
      },
      "personal_harmful": {
        "cohens_d": 2.802734375,
        "accuracy": 0.82,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.470458984375,
        "mean_separation": 0.221435546875
      },
      "medical_ethics": {
        "cohens_d": 2.458984375,
        "accuracy": 0.855,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.408447265625,
        "mean_separation": 0.1668701171875
      },
      "information_deception": {
        "cohens_d": 1.517578125,
        "accuracy": 0.71,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.25048828125,
        "mean_separation": 0.062744140625
      }
    },
    "19": {
      "technical_dangerous": {
        "cohens_d": 1.8349609375,
        "accuracy": 0.78,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.323974609375,
        "mean_separation": 0.10491943359375
      },
      "social_political": {
        "cohens_d": 1.6630859375,
        "accuracy": 0.69,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.30322265625,
        "mean_separation": 0.0921630859375
      },
      "personal_harmful": {
        "cohens_d": 2.91796875,
        "accuracy": 0.735,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.53515625,
        "mean_separation": 0.285888671875
      },
      "medical_ethics": {
        "cohens_d": 2.6015625,
        "accuracy": 0.8,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.46728515625,
        "mean_separation": 0.21826171875
      },
      "information_deception": {
        "cohens_d": 1.5869140625,
        "accuracy": 0.635,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.282470703125,
        "mean_separation": 0.0797119140625
      }
    },
    "20": {
      "technical_dangerous": {
        "cohens_d": 2.19140625,
        "accuracy": 0.795,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.47900390625,
        "mean_separation": 0.2294921875
      },
      "social_political": {
        "cohens_d": 2.1953125,
        "accuracy": 0.685,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.49951171875,
        "mean_separation": 0.25
      },
      "personal_harmful": {
        "cohens_d": 3.353515625,
        "accuracy": 0.77,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.77001953125,
        "mean_separation": 0.5927734375
      },
      "medical_ethics": {
        "cohens_d": 2.9765625,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.66748046875,
        "mean_separation": 0.445556640625
      },
      "information_deception": {
        "cohens_d": 1.828125,
        "accuracy": 0.65,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.399658203125,
        "mean_separation": 0.15966796875
      }
    },
    "21": {
      "technical_dangerous": {
        "cohens_d": 1.8642578125,
        "accuracy": 0.835,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.321533203125,
        "mean_separation": 0.103515625
      },
      "social_political": {
        "cohens_d": 1.5888671875,
        "accuracy": 0.665,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.27392578125,
        "mean_separation": 0.074951171875
      },
      "personal_harmful": {
        "cohens_d": 2.693359375,
        "accuracy": 0.78,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.49658203125,
        "mean_separation": 0.24658203125
      },
      "medical_ethics": {
        "cohens_d": 2.40625,
        "accuracy": 0.865,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.42724609375,
        "mean_separation": 0.1827392578125
      },
      "information_deception": {
        "cohens_d": 1.478515625,
        "accuracy": 0.665,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.25244140625,
        "mean_separation": 0.063720703125
      }
    },
    "22": {
      "technical_dangerous": {
        "cohens_d": 1.958984375,
        "accuracy": 0.83,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.377197265625,
        "mean_separation": 0.1422119140625
      },
      "social_political": {
        "cohens_d": 1.837890625,
        "accuracy": 0.66,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.355224609375,
        "mean_separation": 0.126220703125
      },
      "personal_harmful": {
        "cohens_d": 2.830078125,
        "accuracy": 0.81,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.58056640625,
        "mean_separation": 0.3369140625
      },
      "medical_ethics": {
        "cohens_d": 2.578125,
        "accuracy": 0.88,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.513671875,
        "mean_separation": 0.263671875
      },
      "information_deception": {
        "cohens_d": 1.7529296875,
        "accuracy": 0.66,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.3359375,
        "mean_separation": 0.113037109375
      }
    },
    "23": {
      "technical_dangerous": {
        "cohens_d": 1.7626953125,
        "accuracy": 0.86,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.288818359375,
        "mean_separation": 0.0833740234375
      },
      "social_political": {
        "cohens_d": 1.51953125,
        "accuracy": 0.735,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.250732421875,
        "mean_separation": 0.0628662109375
      },
      "personal_harmful": {
        "cohens_d": 2.498046875,
        "accuracy": 0.81,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.43701171875,
        "mean_separation": 0.19091796875
      },
      "medical_ethics": {
        "cohens_d": 2.298828125,
        "accuracy": 0.89,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.387939453125,
        "mean_separation": 0.150634765625
      },
      "information_deception": {
        "cohens_d": 1.5,
        "accuracy": 0.72,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.244384765625,
        "mean_separation": 0.0596923828125
      }
    },
    "24": {
      "technical_dangerous": {
        "cohens_d": 1.5537109375,
        "accuracy": 0.8,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.216552734375,
        "mean_separation": 0.046875
      },
      "social_political": {
        "cohens_d": 1.2021484375,
        "accuracy": 0.92,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.1690673828125,
        "mean_separation": 0.028564453125
      },
      "personal_harmful": {
        "cohens_d": 2.037109375,
        "accuracy": 0.835,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.3017578125,
        "mean_separation": 0.0909423828125
      },
      "medical_ethics": {
        "cohens_d": 2.00390625,
        "accuracy": 0.91,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.287841796875,
        "mean_separation": 0.08282470703125
      },
      "information_deception": {
        "cohens_d": 1.26953125,
        "accuracy": 0.69,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.176513671875,
        "mean_separation": 0.0311279296875
      }
    },
    "25": {
      "technical_dangerous": {
        "cohens_d": 2.53125,
        "accuracy": 0.805,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.54296875,
        "mean_separation": 0.294921875
      },
      "social_political": {
        "cohens_d": 1.9306640625,
        "accuracy": 0.925,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.424560546875,
        "mean_separation": 0.180419921875
      },
      "personal_harmful": {
        "cohens_d": 3.1328125,
        "accuracy": 0.905,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.71826171875,
        "mean_separation": 0.515625
      },
      "medical_ethics": {
        "cohens_d": 3.265625,
        "accuracy": 0.87,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.72412109375,
        "mean_separation": 0.5244140625
      },
      "information_deception": {
        "cohens_d": 1.8291015625,
        "accuracy": 0.795,
        "n_natural": 100,
        "n_artifact": 100,
        "direction_norm": 0.3916015625,
        "mean_separation": 0.1533203125
      }
    }
  },
  "config": {
    "num_layers": 26,
    "key_pattern": "model.layers.{layer}.mlp.output",
    "num_steps": 20
  }
}